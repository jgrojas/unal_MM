---
title: "Ejercicios para desarrollar en R"
author: "JRojas, MRamirez, LRomero"
date: "6/24/2020"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '1'
    toc_float: yes
  bookdown::html_document2:
    number_sections: no
    toc: yes
    toc_depth: 1
    toc_float: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

A continuación se presentan una serie de ejercicios que permitiran interiorizar el manejo de R y su aplicación en operaciones algebraicas. En documento contiene la solución de 25 ejericios que hacen enfasis en el manejo de Algebra Lineal con R.

Previo a la solución de los ejercicios es necesario instalar y cargar las siguientes librerías.

```{r message=FALSE, include=TRUE}
library(scatterplot3d)
library(matrixcalc)
require(MASS)
```

# Punto 1 

### Dada la información siguiente, resuelva:

![](Taller1_img\Ejercicio1.jpg)

### a) Graficar los dos vectores

```{r}

#Vectores de entrada
x=c(5,1,3)
y=c(-1,3,1)

#Función para graficar los vectores
scatterplot3d(x,y)

```

### b) Encontrar a partir de los vectores datos: i) largo de x; ii) ángulo (x, y)

```{r}

#Cálculo del largo de x: Recordar el teorema de Pitagoras
lgx=sqrt(5^2+1^2+3^2) 
lgy=sqrt((-1)^2+3^2+1^2)

#Cálculo del ángulo entre dos vectors a partir del producto punto

# 1) Cálculo de la transpuesta de los vectores  

xt=t(c(5,1,3))
yt=t(c(-1,3,1))

# 2) Cálculo del producto punto

pp=xt%*%y;

# 3) Cálculo del ángulo 
cosfi=pp/(lgx*lgy)
fi=acos(cosfi);fi

```

### c) 




# Punto 2

### Dada las siguiente matrices

\[A=\begin{bmatrix}-1 & 3\\ 4 & 2\end{bmatrix}, B=\begin{bmatrix}4  & -3 \\ 1 & -2 \\ -2 & 0\end{bmatrix}, C=\begin{bmatrix}5 \\ -4 \\ 2\end{bmatrix}\]

```{r}

A = matrix(c(-1,3,4,2), ncol=2, byrow=TRUE)
B = matrix(c(-4,-3,1,-2,-2,0), ncol=2, byrow=TRUE)
C = matrix(c(5,-4,2),ncol=1,byrow = TRUE)

```


### Realizar las siguientes multiplicaciones

### a) $5A$
```{r}
5*A
```

### b) $BA$

```{r}
B%*%A
```

### c) $A^TB^T$

```{r}
t(A)%*%t(B)
```

### d) $C^TB$

```{r}
t(C)%*%B
```

### e) $AB$ está definida?

```{r message=FALSE, warning=TRUE}
try(A%*%B)
#No está definida por la dimensión de las matrices
```

# Punto 3

### Verifique las siguientes propiedades de la matriz transpuesta cuando:

\[A=\begin{bmatrix}2  & 1\\ 1 & 3\end{bmatrix}, B=\begin{bmatrix}1  & 4 & 2\\ 5 & 0 & 3\end{bmatrix}, C=\begin{bmatrix}1  & 4\\ 3 & 2\end{bmatrix}\]

```{r}
A = matrix(c(2,1,1,3), ncol=2, byrow=TRUE)
B = matrix(c(1,4,2,5,0,3), ncol=3, byrow=TRUE)
C = matrix(c(1,4,3,2), ncol=2, byrow=TRUE)
```

### a) $(A^T)^T=A$

```{r}
t(t(A)) #Es la misma A
```

### b) $(C^T)^-1=(C^-1)^T$

```{r}
D=matrix.inverse(t(C));D 
E=t(matrix.inverse(C));E
# Son de igual resultado, sin envargo al aplicar el validador lógico de R aparece que son diferentes
D==E
```

### c) $(AB)^T=A^TB^T$
```{r}
D=t(A%*%B)
E=t(B)%*%t(A)
D==E # El restultado es que son iguales 
```


# Punto 4

### Cuando $A^-1$ and $B^-1$ existen, pruebe lo siguiente:


### a) $(A^T)^-1=(A^-1)^T$

```{r}
matrix.inverse(t(A))==t(matrix.inverse(A)) #Correcto, la validación es verdadera 

``` 

### b) $(AB)^-1=B^-1A^-1$

```{r}
try(matrix.inverse(A%*%B)==(matrix.inverse(B))%*%(matrix.inverse(A))) # La matrix B no es cuadrada
```

# Punto 5

### Verifique que:

\[Q=\begin{bmatrix}\frac{5}{13}  & \frac{12}{13}\\ -\frac{12}{13} & \frac{5}{13}\end{bmatrix}\]



### Es una matriz ortogonal

```{r}

Q=matrix(c(5/13,12/13,-(12/13),5/13),nrow = 2, byrow = TRUE)
#Para probar se multiplica por la transpuesta
Q%*%t(Q) 
#Como el resultado es la idéntica se define que es ortogonal

```

# Punto 6. 

Dada la matriz $A$: 

\begin{bmatrix}
9 & -2\\
2 & 6
\end{bmatrix}

Indicar si:
 * Simétrica
 * Definida Positiva

```{r}
A<-matrix(c(9,-2,-2,6), ncol=2, byrow=TRUE)

### SIMETRIA

A == t(A)
```

Es simétrica debido a que la matriz es igual a su transpuesta.

Para indicar si es positiva se definen los vectores $g$, $m$ y $n$ los cuales corresponden a tres combinaciones del vector:

\begin{bmatrix}
8\\
5
\end{bmatrix}

Para realizar la multiplicación de su trasnpuesto con la matriz $A$ con el vector, como se observa a continuación:

```{r}
### MATRIZ POSITIVA

g<-c(-8,-5)
m<-c(8,-5)
n<-c(-8,5)

t(g)%*%A%*%g
t(m)%*%A%*%m
t(n)%*%A%*%n
```

Se observa que con las tres vectores, el resultado es un valor positivo, por lo que si se puede asumir que la matriz se define positiva.

# Punto 7. 

Dada la matriz del punto 6 definir:

* Determinar los valores y vectores propios
* Encontrar la inversa
* Encontrar los valores y vectores propios de la matriz inversa

```{r}
eigen(A)
```

Con la función eigen se obtiene los valores y vectores propios de la matriz $A$

```{r}
solve(A)
```

Con la función solve se obtiene la matriz inversa de $A$

```{r}
eigen(solve(A))
```

Con la función eigen y solve se obtiene los valores y vectores propios de la matriz inversa de $A$


# Punto 8. 

Dada la matriz $A$: 

\begin{bmatrix}
1 & 2\\
2 & -2
\end{bmatrix}

Encontrar:
 * Matriz Inversa
 * Valores y vectores propios de la matriz inversa
```{r}

A<-matrix(c(1,2,2,-2), ncol=2, byrow=TRUE)

solve(A)
eigen(solve(A))
```


# Punto 10.

Dada la matriz $A$: 

\begin{bmatrix}
4 & 4.001\\
4.001 & 4.002
\end{bmatrix}

Dada la matriz $B$: 

\begin{bmatrix}
4 & 4.001\\
4.001 & 4.002001
\end{bmatrix}

Se observa que en la posición $(2,2)$ poseen una diferencia significativamente pequeña, sin embargo las columnas de las dos columnas son muy cercanas a ser dependientes. Demuestre que $A^{-1} = -3B^{-1}$ con el propósito de tener en cuenta que los resultados cambian en estas situaciones.

```{r}
A<-matrix(c(4,4.001,4.001,4.002), ncol=2, byrow=TRUE)
B<-matrix(c(4,4.001,4.001,4.002001), ncol=2, byrow=TRUE)

solve(A) == -3*solve(B)
solve(A)
-3*solve(B)
```

Apesar de que el operador lógico indica que los valores no son iguales, al observar las dos matrices por separado, se ve que se puede considerar que son identicas.

# Punto 11. 
```{r}

A<-matrix(c(5,0,0,0,0,1,0,0,0,0,3,0,0,0,4), ncol=4, byrow=TRUE)

### Funcion para multiplicar elementos de un vector

product <- function(vec){
  out <- 1
  for(i in 1:length(vec)){
    out <- out*vec[i]
  }
  out
}

det(A)
diag(A)  

det(A) == product(diag(A))

```

# Punto 12. 
```{r}
A<-matrix(c(0,1,2,3,1,0,4,5,2,4,0,6,3,5,6,0), ncol=4, byrow=TRUE)

d<-eigen(A)

det(A) == product(d$values)
```

# Punto 13.

Indicar si $3x_1^{2} + 3x_2^{2} - 2x_1x_2$ es una forma cuadrática.

Como primera medida se construye la matriz $A$:

```{r}
A<-matrix(c(3,-1,-1,3), ncol=2, byrow=TRUE)
```

Se valida que el determinante de la matriz $A$ sea igual a la multiplicación de sus valores propios:

```{r}
d<-eigen(A)
det(A) == product(d$values)
det(A)
product(d$values)
```

Se toma el mismo ejemplo del punto 6 para validar si es una forma cuadrática:

```{r}

g<-c(-8,-5)
m<-c(8,-5)
n<-c(-8,5)

t(g)%*%A%*%g
t(m)%*%A%*%m
t(n)%*%A%*%n
```

El resultado indica que si se puede considerar como una.

# Punto 14. 

Dada la matriz $A$: 

\begin{bmatrix}
4 & 0 & 0\\
0 & 9 & 0\\
0 & 0 & 1
\end{bmatrix}

Encontrar:
 * $\sum^{-1}$
 * Los valores propios de $\sum$
 * Los valores propios de $\sum^{-1}$

```{r}
A<-matrix(c(4,0,0,0,9,0,0,0,1), ncol=3, byrow=TRUE)

d<-var(A) ##Matriz Varianza y Covarianza

eigen(d)

Z=matrix(c(4,0,0,0,9,0,0,0,1), nrow = 3, byrow = TRUE)

matrix.inverse(Z)

eigen(Z)
eigen(matrix.inverse(Z))
```

# Punto 15. 

Dada la matriz $\sum$: 

\begin{bmatrix}
25 & -2 & 4\\
-2 & 4 & 1\\
4 & 1 & 9
\end{bmatrix}

Encontrar:
 * Las matrices $\rho$ y $V^{1/2}$
 * Demuestre que $V^{1/2} \rho V^{1/2} = \sum$

```{r}
## A

A<-matrix(c(25,-2,4,-2,4,1,4,1,9), ncol=3, byrow=TRUE)
desv<-diag(diag(A^0.5),3,3) ##Matriz Desviacion Estándar
desv
cor<-solve(desv)%*%A%*%solve(desv) ## Matriz de Correlacción
cor

## B

desv%*%cor%*%desv
```


######################### LUCHO

#Punto 17

```{r}

# Una matriz es idempotente cuando es igual a su cuadrado, es decir A*A = A
A17 <- matrix(c(2,-1,1,-2,3,-2,-4,4,-3), ncol= 3)
A17

A17A17 <- A17%*%A17
A17A17

A17 == A17A17

#RESPUESTA: Efectivamente la matriz A17 es idempotente
```

#Punto 16

```{r}
A16 <- matrix(c(1,2,-3,-2,3,1,3,-1,2), ncol= 3)
A16

B16 <- matrix(c(1,0,1,0,1,2,2,2,0), ncol= 3)
B16

A16B16 <- A16%*%B16
A16B16

B16A16 <- B16%*%A16
B16A16

A16B16 == B16A16
# RESPUESTA: efectivamente los resultados cambian, la multiplicación de matrices no es conmutativa
```


#Punto 18

```{r}
A18 <- matrix(c(12,8,8,12,5,6,3,4,-3,4,-1,2,2,6,-1,4), ncol= 4)
A18

A18det <- det(A18)
A18det # 616
# Efectivamente la respuesta es 616
```


#Punto 19

```{r}
A19 <- matrix(c(1,2,3,2,4,1,1,6,2), ncol= 3)
A19

B19 <- matrix(c(-1,2,3,2,-1,4,-3,4,1), ncol= 3)
B19

A19B19 <- A19%*%B19
A19B19

A19B19det <- det(A19B19)
A19B19det #80

A19det <- det(A19)
A19det

B19det <- det(B19)
B19det

A19det*B19det#80

#RESPUESTA: se cumple la relación y en ambos casos el resultado es 80
```


#Punto 20

```{r}
A20 <- matrix(c(-1,4,4,2,-3,-4,-2,4,5), ncol= 3)
A20

A20inv <- solve(A20)
A20inv

# En este caso se cumple con la igualdad
```



#Punto 21

```{r}
A21 <- matrix(c(1,2,1,-1,-1,0,1,0,1), ncol= 3)
A21

# Pruebe que A^2 = A^-1

A21A21 <- A21%*%A21
A21A21

A21inv <- solve(A21)
A21inv

A21A21 == A21inv

# El cuadrado de una matriz no es equivalente a su inversa
```


#Punto 22

```{r}
A22 <- matrix(c(2,1,3,1,1,2,3,-1,3,-1,2,4,1,4,5,-3), ncol= 4)
A22

#A22inv <- solve(A22)

# RESPUESTA, la matriz es singular, por lo que no es invertible

A22vv <- eigen(A22)
A22vv
rA22 <- qr(A22)
rA22$rank

# RESPUESTA, dado que la matriz cuadrada tiene dos valores propios diferentes a 0, entonces el rango
# de la matriz es de 2. Esto se comprueba con el comando qr() y el paquete matlib
```


#Punto 23
```{r}
A23 <- matrix(c(1,2,3,1,-3,-2,-1,4,3), ncol= 3)
A23

B23 <- matrix(c(-1,6,5,-1,12,10,-1,6,5), ncol= 3)
B23

A23B23 <- A23%*%B23
A23B23
rA23B23 <- qr(A23B23) 
rA23B23$rank # Rango 1

B23A23 <- B23%*%A23
B23A23
rB23A23 <- qr(B23A23)
rB23A23$rank # Rango 2

# RESPUESTA: efectivamente, los rangos son diferentes, demostrando nuevamente que la multiplicación
# de matrices no es conmutativa.
```



#Punto 24

```{r}
# Recuerda: EL rango de una matriz corresponde al número de filas o columnas linealmente independientes
A24 <- matrix(c(6,4,10,16,1,2,3,4,3,6,9,12,8,-1,7,15), ncol= 4)
A24
rA24 <- qr(A24) 
rA24$rank # Rango 2

B24 <- matrix(c(1,2,5,2,3,8,3,4,11,4,5,14,5,1,7), ncol= 5)
B24
rB24 <- qr(B24) 
rB24$rank # Rango 2
```


# Punto 25. 
```{r}

A25 <- matrix(c(1,3,2,2,6,4), ncol=3, byrow=TRUE)
A25

G251 <- matrix(c(1,0,0,0,0,0), ncol=2, byrow=TRUE)
G251

G252 <-matrix(c(-42,-1,5,3,2,2), ncol=2, byrow=TRUE)
G252

#Para saber si corresponden a inversas generalizadas, debe cumplirse la propiedad AA-A= A, siendo A- cualquiera de las matrices inversas propuestas.

A25%*%G251%*%A25 == A25
A25%*%G252%*%A25 == A25

#Dado que se cumple la propiedad, podemos afirmar que las matrices G251 y G252 son inversas generalizadas de A25 
```

































